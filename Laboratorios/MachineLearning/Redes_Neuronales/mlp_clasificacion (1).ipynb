{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7n4T1NsMDPFp"
      },
      "source": [
        "# El Perceptrón Multicapa - Clasificación"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "''' NOMBRE: Sebastian Gonzales Tito\n",
        "CARRERA: Ing. Sistemas\n",
        "\n",
        "\n",
        "Modelo para prediccion de numeros con Redes Neuronales usando Modelos de Clasificacion Multicapa\n",
        "Usando como activacion el algoritmo RELU y tambien crossentropy.\n",
        "\n",
        "Cambiando valores de Capas Ocultas, Numero de iteraciones y calor de coeficiente \n",
        "'''"
      ],
      "metadata": {
        "id": "EiG1Jdf-YshT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nz2K4jn2DPFr"
      },
      "source": [
        "## El Perceptrón Multicapa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQxcLo_vDPFt"
      },
      "source": [
        "En el [post](https://sensioai.com/blog/023_mlp_backprop) anterior hemos introducido el modelo de `Perceptrón Multicapa`, o MLP, la arquitectura de red neuronal más básica basada en el [Perceptrón](https://sensioai.com/blog/012_perceptron1). Hemos visto cómo calcular la salida de un MLP de dos capas a partir de unas entradas y cómo encontrar los pesos óptimos para una tarea de regresión. En este post vamos a mejorar la implementación de nuestro MLP de dos capas para que sea capaz también de llevar a cabo tareas de clasificación.\n",
        "\n",
        "![](https://www.researchgate.net/profile/Mohamed_Zahran6/publication/303875065/figure/fig4/AS:371118507610123@1465492955561/A-hypothetical-example-of-Multilayer-Perceptron-Network.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9RfGFDptDPFu"
      },
      "source": [
        "## Implementación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozIFxmHHDPFv"
      },
      "source": [
        "La mayoría del código que utilizaremos fue desarrollado para el `Perceptrón` y lo puedes encontrar en este [post](https://sensioai.com/blog/018_perceptron_final). Lo único que cambiaremos es la lógica del modelo, el resto de funcionalidad (funciones de pérdida, funciones de activación, etc, siguen siendo exactamente igual)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSXpIU6cDPFv"
      },
      "source": [
        "### Funciones de activación"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_emNm7gzDPFw"
      },
      "source": [
        "Para la capa oculta de nuestro MLP utilizaremos una función de activación de tipo `relu`, de la cual necesitaremos su derivada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.041393Z",
          "start_time": "2020-08-05T16:49:53.024396Z"
        },
        "id": "zA6TH8QYDPF0"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "  return np.maximum(0, x)\n",
        "\n",
        "def reluPrime(x):\n",
        "  return x > 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "387gsk8wDPGA"
      },
      "source": [
        "En cuanto a las funciones de activación que utilizaremos a la salida del MLP, éstas son las que hemos introducido en posts anteriores:\n",
        "\n",
        "- Lineal: usada para regresión (junto a la función de pérdida MSE).\n",
        "- Sigmoid: usada para clasificación binaria (junto a la función de pérdida BCE).\n",
        "- Softmax: usada para clasificación multiclase (junto a la función de pérdida crossentropy, CE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.311543Z",
          "start_time": "2020-08-05T16:49:53.298548Z"
        },
        "id": "Sp2EDONoDPGB"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    return np.exp(x) / np.exp(x).sum(axis=-1,keepdims=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTHe1f-kDPGC"
      },
      "source": [
        "### Funciones de pérdida"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOnUGjjpDPGF"
      },
      "source": [
        "Como acabamos de comentar en la sección anterior, estas son las funciones de pérdida que hemos visto hasta ahora para las diferentes tareas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.677416Z",
          "start_time": "2020-08-05T16:49:53.673343Z"
        },
        "id": "BMfHi9CtDPGG"
      },
      "outputs": [],
      "source": [
        "# Mean Square Error -> usada para regresión (con activación lineal)\n",
        "def mse(y, y_hat):\n",
        "    return np.mean((y_hat - y.reshape(y_hat.shape))**2)\n",
        "\n",
        "# Binary Cross Entropy -> usada para clasificación binaria (con sigmoid)\n",
        "def bce(y, y_hat):\n",
        "    return - np.mean(y.reshape(y_hat.shape)*np.log(y_hat) - (1 - y.reshape(y_hat.shape))*np.log(1 - y_hat))\n",
        "\n",
        "# Cross Entropy (aplica softmax + cross entropy de manera estable) -> usada para clasificación multiclase\n",
        "def crossentropy(y, y_hat):\n",
        "    logits = y_hat[np.arange(len(y_hat)),y]\n",
        "    entropy = - logits + np.log(np.sum(np.exp(y_hat),axis=-1))\n",
        "    return entropy.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7JsGMU-DPGJ"
      },
      "source": [
        "Y sus derivadas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:53.945375Z",
          "start_time": "2020-08-05T16:49:53.925371Z"
        },
        "id": "xDz__AmRDPGJ"
      },
      "outputs": [],
      "source": [
        "def grad_mse(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_bce(y, y_hat):\n",
        "    return y_hat - y.reshape(y_hat.shape)\n",
        "\n",
        "def grad_crossentropy(y, y_hat):\n",
        "    answers = np.zeros_like(y_hat)\n",
        "    answers[np.arange(len(y_hat)),y] = 1    \n",
        "    return (- answers + softmax(y_hat)) / y_hat.shape[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U26ojsAODPGK"
      },
      "source": [
        "### Implementación MLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5sQW_FRDPGL"
      },
      "source": [
        "Ahora que ya tenemos definidas las diferentes funciones de activación y de pérdida que necesitamos, vamos a implementar nuestro MLP de dos capas capaz de llevar a cabo tanto tareas de regresión como de clasificación. Del mismo modo que ya hicimos con el `Perceptrón`, definiremos una clase base que servirá para la implementación de las clases particulares para cada caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:49:54.342062Z",
          "start_time": "2020-08-05T16:49:54.325063Z"
        },
        "code_folding": [
          3,
          14,
          20,
          55
        ],
        "id": "Iozp0jrHDPGM"
      },
      "outputs": [],
      "source": [
        "# clase base MLP \n",
        "\n",
        "class MLP():\n",
        "  def __init__(self, D_in, H, D_out, loss, grad_loss, activation):\n",
        "    # pesos de la capa 1\n",
        "    self.w1, self.b1 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(D_in+H)),\n",
        "                                  size=(D_in, H)), np.zeros(H)\n",
        "    # pesos de la capa 2\n",
        "    self.w2, self.b2 = np.random.normal(loc=0.0,\n",
        "                                  scale=np.sqrt(2/(H+D_out)),\n",
        "                                  size=(H, D_out)), np.zeros(D_out)\n",
        "    self.ws = []\n",
        "    # función de pérdida y derivada\n",
        "    self.loss = loss\n",
        "    self.grad_loss = grad_loss\n",
        "    # función de activación\n",
        "    self.activation = activation\n",
        "\n",
        "  def __call__(self, x):\n",
        "    # salida de la capa 1\n",
        "    self.h_pre = np.dot(x, self.w1) + self.b1\n",
        "    self.h = relu(self.h_pre)\n",
        "    # salida del MLP\n",
        "    y_hat = np.dot(self.h, self.w2) + self.b2 \n",
        "    return self.activation(y_hat)\n",
        "    \n",
        "  def fit(self, X, Y, epochs = 100, lr = 0.001, batch_size=None, verbose=True, log_each=1):\n",
        "    batch_size = len(X) if batch_size == None else batch_size\n",
        "    batches = len(X) // batch_size\n",
        "    l = []\n",
        "    for e in range(1,epochs+1):     \n",
        "        # Mini-Batch Gradient Descent\n",
        "        _l = []\n",
        "        for b in range(batches):\n",
        "            # batch de datos\n",
        "            x = X[b*batch_size:(b+1)*batch_size]\n",
        "            y = Y[b*batch_size:(b+1)*batch_size] \n",
        "            # salida del perceptrón\n",
        "            y_pred = self(x) \n",
        "            # función de pérdida\n",
        "            loss = self.loss(y, y_pred)\n",
        "            _l.append(loss)        \n",
        "            # Backprop \n",
        "            dldy = self.grad_loss(y, y_pred) \n",
        "            grad_w2 = np.dot(self.h.T, dldy)\n",
        "            grad_b2 = dldy.mean(axis=0)\n",
        "            dldh = np.dot(dldy, self.w2.T)*reluPrime(self.h_pre)      \n",
        "            grad_w1 = np.dot(x.T, dldh)\n",
        "            grad_b1 = dldh.mean(axis=0)\n",
        "            # Update (GD)\n",
        "            self.w1 = self.w1 - lr * grad_w1\n",
        "            self.b1 = self.b1 - lr * grad_b1\n",
        "            self.w2 = self.w2 - lr * grad_w2\n",
        "            self.b2 = self.b2 - lr * grad_b2\n",
        "        l.append(np.mean(_l))\n",
        "        # guardamos pesos intermedios para visualización\n",
        "        self.ws.append((\n",
        "            self.w1.copy(),\n",
        "            self.b1.copy(),\n",
        "            self.w2.copy(),\n",
        "            self.b2.copy()\n",
        "        ))\n",
        "        if verbose and not e % log_each:\n",
        "            print(f'Epoch: {e}/{epochs}, Loss: {np.mean(l):.5f}')\n",
        "\n",
        "  def predict(self, ws, x):\n",
        "    w1, b1, w2, b2 = ws\n",
        "    h = relu(np.dot(x, w1) + b1)\n",
        "    y_hat = np.dot(h, w2) + b2\n",
        "    return self.activation(y_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T16:57:40.536617Z",
          "start_time": "2020-08-05T16:57:40.515590Z"
        },
        "id": "CkmurgmHDPGP"
      },
      "outputs": [],
      "source": [
        "# MLP para regresión\n",
        "class MLPRegression(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, mse, grad_mse, linear)\n",
        "\n",
        "# MLP para clasificación binaria\n",
        "class MLPBinaryClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, bce, grad_bce, sigmoid)\n",
        "\n",
        "# MLP para clasificación multiclase\n",
        "class MLPClassification(MLP):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        super().__init__(D_in, H, D_out, crossentropy, grad_crossentropy, relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uktPtOuDPGQ"
      },
      "source": [
        "Vamos a probar ahora nuestra implementación para diferentes ejemplos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBMyLPG9DPGz"
      },
      "source": [
        "## Clasificación Multiclase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crzRO7d5DPGz"
      },
      "source": [
        "Por último vamos a ver cómo aplicar nuestro modelo para clasificación en multiples clases."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.io import loadmat\n",
        "import os\n",
        "from matplotlib import pyplot"
      ],
      "metadata": {
        "id": "rcDH2VY_Pol1"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:05:47.527647Z",
          "start_time": "2020-08-05T17:05:47.393648Z"
        },
        "id": "T4WcdCNbDPG0",
        "outputId": "df4e0d5b-3f20-4e50-dfd9-687edb235ee2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[10 10 10 ...  9  9  9]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" X = iris.data[:, (2, 3)]  # petal length, petal width\n",
        "y = iris.target\n",
        "\n",
        "X_mean, X_std = X.mean(axis=0), X.std(axis=0)\n",
        "X_norm = (X - X_mean) / X_std\n",
        "\n",
        "plt.plot(X[y==0, 0], X[y==0, 1], 's', label=\"Iris Setosa\")\n",
        "plt.plot(X[y==1, 0], X[y==1, 1], 'x', label=\"Iris Versicolor\")\n",
        "plt.plot(X[y==2, 0], X[y==2, 1], 'o', label=\"Iris Virginica\")\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.xlabel('petal length', fontsize=14)\n",
        "plt.ylabel('petal width', fontsize=14)\n",
        "plt.title(\"Iris dataset\", fontsize=14)\n",
        "plt.show() \"\"\"\n",
        "data = loadmat(os.path.join('ex3data1.mat'))\n",
        "X, y = data['X'], data['y'].ravel()\n",
        "print(y)\n",
        "y[y == 10] = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def displayData(X, example_width=None, figsize=(10, 10)):\n",
        "    \"\"\"\n",
        "    Muestra datos 2D almacenados en X en una cuadrícula apropiada.\n",
        "    \"\"\"\n",
        "    # Calcula filas, columnas\n",
        "    if X.ndim == 2:\n",
        "        m, n = X.shape\n",
        "    elif X.ndim == 1:\n",
        "        n = X.size\n",
        "        m = 1\n",
        "        X = X[None]  # Promocionar a una matriz bidimensional\n",
        "    else:\n",
        "        raise IndexError('La entrada X debe ser 1 o 2 dimensinal.')\n",
        "\n",
        "    example_width = example_width or int(np.round(np.sqrt(n)))\n",
        "    example_height = n / example_width\n",
        "\n",
        "    # Calcula el numero de elementos a mostrar\n",
        "    display_rows = int(np.floor(np.sqrt(m)))\n",
        "    display_cols = int(np.ceil(m / display_rows))\n",
        "\n",
        "    fig, ax_array = pyplot.subplots(display_rows, display_cols, figsize=figsize)\n",
        "    fig.subplots_adjust(wspace=0.025, hspace=0.025)\n",
        "\n",
        "    ax_array = [ax_array] if m == 1 else ax_array.ravel()\n",
        "\n",
        "    for i, ax in enumerate(ax_array):\n",
        "        ax.imshow(X[i].reshape(example_width, example_width, order='F'),\n",
        "                  cmap='Greys', extent=[0, 1, 0, 1])\n",
        "        ax.axis('off')\n",
        "\n",
        "def correccionPredict(predic):\n",
        "  L_mayor = []\n",
        "  for value in predic:\n",
        "    mayor = 0\n",
        "    for index in range(len(value)):\n",
        "      if(value[index] > value[mayor]):\n",
        "        mayor = index\n",
        "    L_mayor.append(mayor)\n",
        "\n",
        "  return L_mayor"
      ],
      "metadata": {
        "id": "PAZTrdiPQGsY"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-08-05T17:08:01.961788Z",
          "start_time": "2020-08-05T17:08:01.891163Z"
        },
        "id": "Jn0cDadSDPHK",
        "outputId": "631a6f74-9216-4304-a14e-55d4d3dfd31f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 200/2000, Loss: 0.72576\n",
            "Epoch: 400/2000, Loss: 0.51941\n",
            "Epoch: 600/2000, Loss: 0.43094\n",
            "Epoch: 800/2000, Loss: 0.37793\n",
            "Epoch: 1000/2000, Loss: 0.34091\n",
            "Epoch: 1200/2000, Loss: 0.31268\n",
            "Epoch: 1400/2000, Loss: 0.28996\n",
            "Epoch: 1600/2000, Loss: 0.27103\n",
            "Epoch: 1800/2000, Loss: 0.25484\n",
            "Epoch: 2000/2000, Loss: 0.24074\n"
          ]
        }
      ],
      "source": [
        "model = MLPClassification(D_in=400, H=250, D_out=10)\n",
        "epochs, lr = 2000, 0.1\n",
        "model.fit(X, y, epochs, lr, batch_size=4999, log_each=200)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_prueba = X[1899:1999, :].copy()\n",
        "prediccionModelo = model.predict([model.w1, model.b1, model.w2, model.b2], X_prueba)\n",
        "print( 'El entrenamiento tiene una precision de {:.2f}%'.format(np.mean(correccionPredict(prediccionModelo) == y[1899:1999])* 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjXQ6u7lSHTy",
        "outputId": "45c9bebb-ebe7-4905-bac4-ddc1ccc1457d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El entrenamiento tiene una precision de 95.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_prueba = X[1899:1900, :].copy()\n",
        "print(y[1900])\n",
        "prediccionModelo = model.predict([model.w1, model.b1, model.w2, model.b2], X_prueba)\n",
        "print(prediccionModelo)\n",
        "displayData(X_prueba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "id": "OfmwTsKZSjS0",
        "outputId": "f0234dcf-2b9c-45f8-d8e7-b9a27351dd12"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "[[0.         2.62924024 0.76549085 9.3670391  0.         2.54646465\n",
            "  0.         0.         5.1111372  0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygqz2WiEDPHL"
      },
      "source": [
        "De nuevo, nuestro MLP es capaz de separar las tres clases en el dataset. En posts anteriores hemos trabajado también con el dataset MNIST para clasificación de imágenes en diez clases distintas. ¿Te ves capaz de utilizar nuestro MLP para resolver ese problema?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoVtbkriDPHM"
      },
      "source": [
        "## Resumen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E293vNsLDPHN"
      },
      "source": [
        "En este post hemos visto como implementar un `Perceptrón Multicapa` en `Python` para tareas de regresión y clasificación. Como ya hicimos anteriormente para el caso del `Perceptrón` hemos validado nuestra implementación con el dataset de clasificación de flores *Iris*, tanto para clasificación binaria como multiclase. Sin embargo, nuestra implementación está muy limitada. ¿Qué pasa si queremos usar un MLP de más de dos capas?, ¿y si queremos usar una función de activación diferente a la `relu` en la capa oculta?, ¿podríamos utilizar un algoritmo de optimizaciñon diferente al `descenso por gradiente`? Para poder hacer todo esto necesitamos un `framework` más flexible, similar a lo que nos ofrecen `Pytorch` y `Tensorflow`. En el siguiente post desarrollaremos nuestro propio framework de MLP para que sea más flexible y que también nos servirá para entender cómo funcionan el resto de frameworks de `redes neuronales` por dentro."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {
        "height": "calc(100% - 180px)",
        "left": "10px",
        "top": "150px",
        "width": "233.594px"
      },
      "toc_section_display": true,
      "toc_window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "3bae3d8c463296fe75c902895712520123bdee03fdc749cca1f2f6fed1a18ba7"
      }
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}